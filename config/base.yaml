base:
  drop_list: ['arrival_date_year',
              'arrival_date_month',
  #            'arrival_date_week_number',
  #            'arrival_date_day_of_month',
  #            'market_segment',
  #            'company',
  #            'is_repeated_guest',
              'ID',
              'reservation_status_date',
              'reservation_status']
  # this feature selection is suggested in [https://medium.com/analytics-vidhya/exploratory-data-analysis-of-the-hotel-booking-demand-with-python-200925230106]
  #drop_list: ['ID',
  #            'arrival_date_year',
  #            'arrival_date_week_number',
  #            'arrival_date_day_of_month',
  #            'arrival_date_month',
  #            'assigned_room_type',
  #            'reserved_room_type',
  #            'reservation_status_date',
  #            'reservation_status',
  #            'previous_cancellations',
  #            'previous_bookings_not_canceled']

# ==============================
# Regression
# ==============================
RFR:
  n_jobs: 6
  criterion: 'mae'
  n_estimators: 100
  verbose: True

SVR:
  # kernel{'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'
  kernel: 'rbf'
  # gamma{'scale', 'auto'} or float, default='scale'
  gamma: 'scale'
  # tolerance: float, default=1e-3 Tolerance for stopping criterion.
  tol: 0.001
  C: 10000
  # Epsilon in the epsilon-SVR model. 
  # It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.
  epsilon: 0.1
  verbose: True


# ==============================
# Classification (cancel model)
# ==============================

# is_cancel: {0.0: 47062, 1.0: 26357}
# classification weight: {0.0: 1, 1.0: 1.7855598133323216} (47062/26357)

RanForestC:
  n_jobs: 6
  # 'gini', 'entropy'
  criterion: 'entropy'
  # class_weight: {"balanced", "balanced_subsample"}, default=None
  # {0.0: 1.0, 1.0: 50.0}
  class_weight: "balanced"
  n_estimators: 100
  random_state: 1126
  verbose: True

AdaBoostC:
  # The base estimator from which the boosted ensemble is built.
  # Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes.
  # If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.
  base_estimator: ~
  # CER 
  # n_estimators
  # 50: 0.217 / 100: 0.208
  n_estimators: 150
  learning_rate: 1
  # algorithm{'SAMME', 'SAMME.R'}, default='SAMME.R'
  algorithm: 'SAMME.R'
  random_state: 1126
  

GraBoostC:
  # loss{'deviance', 'exponential'}, default='deviance'
  loss: 'deviance'
  learning_rate: 0.1

  # CER
  # 100: 0.139
  # 150: 0.141
  n_estimators: 100
  # The fraction of samples to be used for fitting the individual base learners.
  subsample: 1.0
  # The function to measure the quality of a split.
  criterion: 'friedman_mse'
  # Tolerance for the early stopping
  tol: 0.0001
  validation_fraction: 0.2
  random_state: 1126
  verbose: 1

HistGraBoostC:
  # loss{'auto', 'binary_crossentropy', 'categorical_crossentropy'}, default='auto'
  loss: 'binary_crossentropy'
  learning_rate: 0.05
  max_iter: 500
  max_leaf_nodes: ~ # default: 31, ~ => no limit
  validation_fraction: 0.2
  random_state: 1126
  verbose: 1

XGBoostC:
  # parameter usage is here [https://xgboost.readthedocs.io/en/stable/parameter.html]
  # and here [https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn]
  # and here [https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst]
  booster: 'gbtree'
  n_estimators: 100
  learning_rate: 0.1
  verbosity: 3
  random_state: 1126
  # Control the balance of positive and negative weights, useful for unbalanced classes.
  # A typical value to consider: sum(negative instances) / sum(positive instances).
  scale_pos_weight: 1.7855598133323216

lightGBMC:
  # parameter usage [https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier]
  boosting_type: 'gbdt'
  learning_rate: 0.1
  # CER
  # n_estimators: 
  #     100: 0.144/ 150: 0.135 / 200: 0.135
  n_estimators: 150
  n_jobs: 6
  class_weight: 'balanced' #{0.0: 1, 1.0: 50}
  random_state: 1126
  silent: False

BaggingC:
  n_estimators: 50
  bootstrap: True
  # bootstrap_features: whether features are drawn with replacement.
  bootstrap_features: False
  n_jobs: 6
  random_state: 1126
  verbose: 1

MLPC:
  hidden_layer_sizes: !!python/tuple [100,]
  activation: 'relu'
  solver: 'adam'
  batch_size: 'auto'
  # learning_rate{'constant', 'invscaling', 'adaptive'}
  learning_rate: 'constant'
  learning_rate_init: 0.001
  max_iter: 200
  shuffle: True
  random_state: 1126
  verbose: True

SVC:
  # kernel{'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'
  kernel: 'rbf'
  # gamma{'scale', 'auto'} or float, default='scale'
  gamma: 'scale'
  # tolerance: float, default=1e-3 Tolerance for stopping criterion.
  tol: 0.001
  C: 10
  verbose: True


# ==============================
# NN & DP
# ==============================
NN:
  CNN:
    tmp: 5566
  
  LSTM:
    hidden_size: 256
    num_layers: 3
    bidirectional: False
  
  GRU:
    hidden_size: 256
    num_layers: 3
    bidirectional: False
  
  dataloader:
    # training batch size, 12 for pre-train, 6 for cpc exp
    train_batch_size: 6
    eval_batch_size: 12
  
  hyper:
    # Learning rate for opt: ['4e-3' for fine-tune, '4e-3' for regualr downstream task training]
    learning_rate: 2e-4
    # Proportion of training to perform linear rate warmup.
    warmup_proportion: 0.07
    # Maximum gradient norm
    gradient_clipping: 1.0
    # total steps for training, a step is a batch of update
    total_step: 500000