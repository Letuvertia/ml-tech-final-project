base:
  drop_list: ['arrival_date_year', 'arrival_date_month', 'arrival_date_day_of_month', 'is_canceled', 'reservation_status_date', 'reservation_status', 'adr']

RF:
  n_jobs: 6
  criterion: 'mae'
  n_estimators: 100
  verbose: False

SVR:
  tmp: 5566


NN:
  CNN:
    tmp: 5566
  LSTM:
    hidden_size: 256
    num_layers: 3
    bidirectional: False
  GRU:
    hidden_size: 256
    num_layers: 3
    bidirectional: False
  dataloader:
      train_batch_size: 6                                         # training batch size, 12 for pre-train, 6 for cpc exp
      eval_batch_size: 12
  hyper:
      learning_rate: 2e-4                                 # Learning rate for opt: ['4e-3' for fine-tune, '4e-3' for regualr downstream task training]
      warmup_proportion: 0.07                               # Proportion of training to perform linear rate warmup.
      gradient_clipping: 1.0                                # Maximum gradient norm
      total_step: 500000                                            # total steps for training, a step is a batch of update